{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba35a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#milostipanov@gmail.com 2022\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# disable FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "train_data_path='./Training_Data/'\n",
    "output_path='./net/'\n",
    "\n",
    "nnstr='Ozone' #NN name string\n",
    "hiddenlayers=(100,90,75) #set MLNN hidden layers (hidden layers will be reset for aaNN based on number of bands)\n",
    "\n",
    "#radoption=2 #TOA rad data type, options: 1=Lt, 2=Lrc \n",
    "addnoise=0 #Flag for adding Gaussian noise NOTE: do not add noise for forward training\n",
    "noiselevel=0 #Gaussian noise level, e.g. 1=1%\n",
    "\n",
    "#sensor band infos\n",
    "band=[302,312,320,340,380]\n",
    "trainband=np.arange(5)\n",
    "nrrs=5\n",
    "aodidx=np.arange(5)\n",
    "\n",
    "#number of sample in the scatter plot, if total number of data points is less than this, half of the data will be used for plotting\n",
    "nplotsample=5000 \n",
    "\t\n",
    "nband=len(band)\n",
    "ntrainband=len(trainband)\n",
    "naod=len(aodidx) \n",
    "      \n",
    "#read in training data\n",
    "print('Loading training dataset...')\n",
    "for i in np.arange(5):\n",
    "    if i==0:       \n",
    "       par=np.loadtxt(train_data_path +'NILU-training'+str(i+1)+'.txt')\n",
    "    else:\n",
    "       par=np.append(par,np.loadtxt(train_data_path +'NILU-training'+str(i+1)+'.txt'),0)\n",
    "\n",
    "print('Training dataset loaded.')\n",
    "\n",
    "\n",
    "\n",
    "#remove negative values\n",
    "idx=np.where(np.sum(par<0,axis=1)==0)[0]\n",
    "par=par[idx,:]\n",
    "\n",
    "#total number of training cases\n",
    "ncase=len(par)\n",
    "rad=np.zeros((ncase,1))\n",
    "\n",
    "#rad[:,0] = par[:,3]\n",
    "#rad[:,1] = par[:,4]\n",
    "\n",
    "#generate the gaussian noise for each band\n",
    "if addnoise==1:\n",
    "    fname_prefix=nnstr+'WiGN_p'+str(noiselevel)\n",
    "    noise=np.random.normal(1,noiselevel/100,(ncase,1))    # only use 2 ratio\n",
    "    rad=np.multiply(rad,noise) \n",
    "else:\n",
    "    fname_prefix=nnstr+'WoGN' \n",
    "     \n",
    "# NN training: Ozone & Cloud Optical Depth   \n",
    "#trainingoption = itrain + 1\n",
    "\n",
    "nnlayer = hiddenlayers\n",
    "            \n",
    "nlayer=len(nnlayer) # number of hidden layers\n",
    "    \n",
    "#create layer string for file name\n",
    "layerstr=''\n",
    "for i in np.arange(nlayer):\n",
    "    if i<nlayer-1:\n",
    "       layerstr=layerstr+str(nnlayer[i])+'X'\n",
    "    else:\n",
    "       layerstr=layerstr+str(nnlayer[i])\n",
    "\n",
    "trainx=np.zeros((ncase,3))\n",
    "trainy=np.zeros((ncase,2))\n",
    "\n",
    "trainx[:,0]=np.cos(np.deg2rad(par[:,0])) # geometry: cos[Solar Zenith Angle]\n",
    "trainx[:,1]=np.log10(par[:,3]) # Irradiance 380\n",
    "trainx[:,2]=np.log10(par[:,4]) # Ratio\n",
    "\n",
    "trainy[:,0]=np.log10(par[:,1]) # Ozone\n",
    "trainy[:,1]=np.log10(par[:,2]) # Cloud. Vol. Frac.\n",
    "\n",
    "net_name='net_'+fname_prefix+'_ozone'+str(nrrs)+'_'+layerstr+'.h5'\t  \n",
    "for i in np.arange(nrrs):\n",
    "    if i==0:\n",
    "       labelparam=['Rrs'+str(band[i])+'nm']\n",
    "    else:\n",
    "       labelparam=np.append(labelparam,['Rrs'+str(band[i])+'nm'],0)\t\t\t  \n",
    "\n",
    "    \n",
    "    \n",
    "#NORMALIZATION IF NEEDED:\n",
    "\n",
    "# ninput=len(trainx[0])\n",
    "# train_in=np.zeros((ninput,2))\n",
    "# for i in range(ninput):\n",
    "#     train_in[i,0]=trainx[:,i].min()\n",
    "#     train_in[i,1]=trainx[:,i].max()\n",
    "# noutput=len(trainy[0])\t\n",
    "# train_out=np.zeros((noutput,2))\n",
    "# for i in range(noutput):\n",
    "#     train_out[i,0]=trainy[:,i].min()\n",
    "#     train_out[i,1]=trainy[:,i].max()\n",
    "    \n",
    "#normalize the trainx and trainy to [-1,1]\t\n",
    "# for i in range(ninput):\n",
    "#     trainx[:,i]=2*(trainx[:,i]-train_in[i,0])/(train_in[i,1]-train_in[i,0])-1\n",
    "# for i in range(noutput):\n",
    "#     trainy[:,i]=2*(trainy[:,i]-train_out[i,0])/(train_out[i,1]-train_out[i,0])-1\n",
    "\t\n",
    "    \n",
    "\n",
    "\n",
    "#Build MLNN\n",
    "mlnn=MLPRegressor(hidden_layer_sizes=nnlayer,\n",
    "     activation='tanh',\n",
    "\t  solver='adam',\n",
    "    \t  batch_size='auto',\n",
    "    \t  learning_rate='adaptive',\n",
    "    \t  learning_rate_init=0.001,\n",
    "    \t  max_iter=1000,\n",
    "    \t  random_state=5,\n",
    "    \t  tol=1.0e-8,\n",
    "    \t  verbose=True,\n",
    "    \t  early_stopping=True,  \n",
    "    \t  validation_fraction=0.1)\n",
    "#traing MLNN\n",
    "mlnn=mlnn.fit(trainx,trainy)\n",
    "    \n",
    "#save trained NN to HDF5\n",
    "# nn_structure=np.zeros([nlayer+2])\n",
    "# nn_structure[0]=ninput\n",
    "# for i in range(nlayer):\n",
    "#     nn_structure[i+1]=nnlayer[i]\n",
    "# nn_structure[nlayer+1]=noutput\n",
    "    \n",
    "# hf = h5py.File('./net/'+net_name,'w')\n",
    "# hf.create_dataset('Layers',dtype='int8',data = nn_structure)\n",
    "# hf.create_dataset('Norm_in',dtype='float64',data = train_in)\n",
    "# hf.create_dataset('Norm_out',dtype='float64',data = train_out)\n",
    "# gw = hf.create_group('Weights')\n",
    "# for i in range(nlayer+1):    \n",
    "#     gw.create_dataset('Layer'+str(i+1),dtype='float64',data=np.transpose(mlnn.coefs_[i]))\n",
    "# gb = hf.create_group('Bias')\n",
    "# for i in range(nlayer+1):\n",
    "#     gb.create_dataset('Layer'+str(i+1),dtype='float64',data=mlnn.intercepts_[i].reshape(int(nn_structure[i+1]),1))\n",
    "# hf.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530d0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off numpy warning\n",
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "#make prediction using trained MLNN\n",
    "\n",
    "input_data =  np.loadtxt('NILU_Data/processed_nilu-20140602.txt') \n",
    "\n",
    "nline = len(input_data) #length of input file\n",
    "\n",
    "norm_input=np.zeros((nline,3))\n",
    "\n",
    "norm_input[:,0]=np.cos(np.deg2rad(input_data[:,0])) # geometry: cos[Solar Zenith Angle]\n",
    "norm_input[:,1]=np.log10(input_data[:,1]) # Irradiance 380\n",
    "norm_input[:,2]=np.log10(input_data[:,2]) # Ratio\n",
    "\n",
    "\n",
    "#IF USING NORMALIZATION:\n",
    "\n",
    "# ninput=len(norm_input[0])\n",
    "\n",
    "\n",
    "# norm_extr=np.zeros((ninput,2))\n",
    "# for i in range(ninput):\n",
    "#     norm_extr[i,0]=norm_input[:,i].min()\n",
    "#     norm_extr[i,1]=norm_input[:,i].max()\n",
    "\n",
    "    \n",
    "\n",
    "# for i in range(ninput):\n",
    "#     norm_input[:,i]=2*(norm_input[:,i]-norm_extr[i,0])/(norm_extr[i,1]-norm_extr[i,0])-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nnoutput=mlnn.predict(norm_input)\n",
    "\n",
    "\n",
    "\n",
    "#SCALING IT BACK IF NORMALIZATION WAS USED\n",
    "\n",
    "# for i in range(noutput):\n",
    "#     nnoutput[:,i]=(nnoutput[:,i]+1)/2*(train_out[i,1]-train_out[i,0])+train_out[i,0]\n",
    "#     trainy[:,i]=(trainy[:,i]+1)/2*(train_out[i,1]-train_out[i,0])+train_out[i,0]\n",
    "\t\n",
    "nnoutput=10 ** nnoutput\n",
    "trainy=10 ** trainy\n",
    "\n",
    "\n",
    "\n",
    "np.savetxt('nnoutput.txt', nnoutput) \n",
    "\n",
    "\n",
    "#compute average percentage error\n",
    "# diff=(nnoutput-trainy)/trainy*100\n",
    "# ape=np.mean(np.absolute(diff), axis=0)\n",
    "# bias=np.mean(diff, axis=0)\n",
    "    \n",
    "# lim=np.amax(trainy,axis=0)*1.2 #set limit\n",
    "# r2=np.zeros(noutput)\n",
    "# if ncase>nplotsample:\n",
    "#    idx=random.sample(list(range(ncase)),nplotsample)\n",
    "# else:\n",
    "#    idx=random.sample(list(range(ncase)),int(ncase/2))\n",
    "# if noutput<5:\n",
    "#    plt.figure(figsize=(18,5),dpi=150)\n",
    "# else:\n",
    "#    plt.figure(figsize=(18,9),dpi=150)\n",
    "# for i in np.arange(noutput):\n",
    "#    r2[i]=r2_score(trainy[:,i],nnoutput[:,i])\n",
    "#    print(r2[i])\n",
    "#    plt.subplot(2,4,i+1)           \n",
    "#    plt.scatter(trainy[idx,i],nnoutput[idx,i],s=2,c='red')\n",
    "#    plt.xlim(0, lim[i])\n",
    "#    plt.ylim(0, lim[i])\n",
    "#    plt.xlabel('Model '+labelparam[i]) #original line:   plt.xlabel('Model '+labelparam[i])\n",
    "#    plt.ylabel('MLNN '+labelparam[i])  #original line: plt.ylabel('MLNN '+labelparam[i])\n",
    "#    plt.plot([0,lim[i]],[0,lim[i]],'k')\n",
    "#    plt.text(lim[i]*0.05,lim[i]*0.94,('R$^2$ = %0.3f' % (r2[i])))\n",
    "#    plt.text(lim[i]*0.05,lim[i]*0.88,('APE = %0.2f' % (ape[i])+'%'))\n",
    "#    plt.text(lim[i]*0.05,lim[i]*0.82,('Bias = %0.2f' % (bias[i])+'%'))\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(output_path+net_name+'_Training.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
